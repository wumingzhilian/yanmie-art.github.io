---
 layout:   post        # 使用的布局（不需要改）
 title:   python爬虫   # 标题 
 subtitle:   #副标题
 date:    2020-10-03    # 时间
 author:   yanmie       # 作者
 header-img: img/.jpg  ##标签这篇文章标题背景图片
 catalog: true            # 是否归档
 tags:                
    - 工具使用

---

## 一、爬虫技术相关库

网络请求：

* urllib
* requests / urllib3
* selenium ( UI 自动测试、动态js渲染 )
* appium (手机 app 的爬虫或 UI 测试)
* re 正则 
* xpath
* bs4
* json

数据存储：

* pymysql
* mongodb
* elasticsearch

多任务库：

* 多线程 （threading）、线程队列 （queue）
* 协程（asynio、gevent / eventlet）

爬虫框架：

* scrapy
* scrapy-redis 分布式（多机爬虫）

## 二、常见的反爬策略

* UA 策略
* 登录限制（cookie）策略
* 亲求频率（IP代理） 策略
* 验证码（图片 - 云打码，文字或物件图片验证，滑块）
* 动态 js （selenium / splash / api 接口）策略

## 三、绕过UA检测

假如我们要爬取一个网站，就用本地来测试把，先来看看直接爬取的时候中间件记录的日志。

```python
import requests

if __name__ == "__main__":
    # 指定url
    url = 'http://127.0.0.1/'
    # 发起请求
    response = requests.get(url=url)
    # 获取响应数据
    page_text = response.text
    print(page_text)
    with open('./pachong.html','w',encoding='utf-8') as fp:
        fp.write(page_text)
    print("爬取数据结束")
```

可以从日志中看到，ua是 python 库的标志。

```python
127.0.0.1 - - [07/Oct/2020:19:56:34 +0800] "GET / HTTP/1.1" 200 89796 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
127.0.0.1 - - [07/Oct/2020:20:46:19 +0800] "GET / HTTP/1.1" 200 88896 "-" "python-requests/2.24.0"
```

假如网站加入了UA检测策略，这个当然也就爬取不出数据来了。

所以可以伪造UA，既可以成功绕过了。

```python
import requests

if __name__ == "__main__":
    # 指定url
    url = 'http://127.0.0.1/'
    headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    # 发起请求
    response = requests.get(url=url,headers=headers)
    # 获取响应数据
    page_text = response.text
    print(page_text)
    with open('./pachong.html','w',encoding='utf-8') as fp:
        fp.write(page_text)
    print("爬取数据结束")
```

记录结果：

```bash
127.0.0.1 - - [07/Oct/2020:20:46:19 +0800] "GET / HTTP/1.1" 200 88896 "-" "python-requests/2.24.0"
127.0.0.1 - - [07/Oct/2020:20:55:25 +0800] "GET / HTTP/1.1" 200 88951 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
```

## 四、requests库

#### requests之网页采集

```python
import requests

if __name__ == "__main__":
    url = 'https://www.sogou.com/web'
    headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    # 处理 url 携带的参数 ： 封装到字典中
    kw =  input('enter a word: ')
    param = {
        'query':kw
    }
    respose = requests.get(url=url,params=param,headers=headers)
    page_text = respose.text
    print(page_text)
    filename = kw+'.html'
    with open(filename,'w',encoding='utf-8') as fp:
        fp.write(page_text)
    print(filename+'  保存成功！！！')

```

#### requests之百度翻译破解

当进行百度翻译时，我们发现输入但此后，不点翻译按钮，他也会自动显示翻译结果，也就是进行局部刷新( ajax请求 )。

![0dgFTe.png](https://s1.ax1x.com/2020/10/07/0dgFTe.png)

![0dgAFH.png](https://s1.ax1x.com/2020/10/07/0dgAFH.png)

![image-20201007210827117](C:%5CUsers%5C51946%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201007210827117.png)

发送了三个`d`、`do`、`dog`的post包。

我们每录入一个字符就会进行一次ajax请求。

![0dgYpn.png](https://s1.ax1x.com/2020/10/07/0dgYpn.png)

返回一串json数据。

```python

import requests
import json
if __name__ == "__main__":
    post_url = "https://fanyi.baidu.com/sug"
    # UA 伪装
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    # post 请求参数处理（通get）请求一致
    word = input('enter a word: ')
    data = {
        'kw':word
    }
    # 请求发送
    reponse = requests.post(url=post_url,data=data,headers=headers)
    # 获取响应数据
    # 直接打印 josn 数据
    #print(reponse.text)
    # json() 方法返回的是 obj （确认服务器响应数据类型是 josn 数据，才能使用json方法）
    dic_obj = reponse.json()
    print(dic_obj)
    # 进行持久化存储
    fp = open('./'+word+'.json','w',encoding='utf-8')
    json.dump(dic_obj,fp,ensure_ascii=False)
    print('over!')
```

#### requests之豆瓣电影

https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7%E7%89%87&type=24&interval_id=100:90&action=

当滚轮拖动到底部的时候，有变到了中间，就可以继续网下滑了。也是当滑轮到达底部的时候进行了 ajax 请求。

![0dWgcn.png](https://s1.ax1x.com/2020/10/07/0dWgcn.png)

```python
import requests
import json

if __name__ == '__main__':
    url = 'https://movie.douban.com/j/chart/top_list'
    param = {
        'type':'24',
        'interval_id':'100:90',
        'action':'',
        'start':'1',    # 从库中的第几部开始取
        'limit':'20',    # 一次取出来多少
    }
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    reponse = requests.get(url=url,params=param,headers=headers)
    list_data = reponse.json()
    fp = open('./douban.json','w',encoding='utf-8')
    json.dump(list_data,fp=fp,ensure_ascii=False)

    print('over!!!')
```

#### requests之餐厅爬取

这里查询也是 ajax 请求

http://www.kfc.com.cn/kfccda/storelist/index.aspx

![0dIKoV.png](https://s1.ax1x.com/2020/10/07/0dIKoV.png)

```python
import requests

if __name__ == '__main__':
    url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'
    headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    data = {
        'cname':'',
        'pid':'',
        'keyword':'北京',
        'pageIndex':'1',
        'pageSize':'10',
    }
    reponse = requests.post(url=url,data=data,headers=headers)
    list_data = reponse.json()
    print(list_data)

```

#### requests之药监总局



* 需求：爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据 http://scxk.nmpa.gov.cn:81/xk/

```python
import requests
import json

if __name__ == '__main__':
    url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    data={
        'on':'true',
        'page':'1',
        'pageSize':'15',
        'productName':'',
        'conditionType':'1',
        'applyname':'',
        'applysn':''
    }
    json_ids = requests.post(url=url,data=data,headers=headers).json()
    id_list = []    # 存储企业 id
    all_data_list = []   # 存储所有的企业详情数据
    for dict in json_ids['list']:
        # print(dict['ID'])
        id_list.append(dict['ID'])

    post_url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
    for id in id_list:
        # print(id)
        data ={
            'id':id
        }
        detail_json = requests.post(url=post_url,data=data,headers=headers).json()
        all_data_list.append(detail_json)
    # print(all_data_list)
    fp = open('./test/allData.json','w',encoding='utf-8')
    json.dump(all_data_list,fp=fp,ensure_ascii=False)
    print('over!!!')
```

## 五、数据解析

聚焦爬虫：爬取页面中指定页面内容。

编码流程：

* 指定 url
* 发起请求
* 获取响应数据
* 数据解析
* 持久化存储

数据解析分类：

* 正则
* bs4
* xpath (***)

数据解析原理概述

* 解析的局部文本内容都会在标签或者标签对应的属性中存储
  * 进行指定标签的定位
  * 标签或者标签对应的属性中存储的数据值进行提取（解析）

#### 1. 数据解析--正则表达式

![0w06F1.png](https://s1.ax1x.com/2020/10/08/0w06F1.png)

#### 2. 图片爬取

```python
import requests
if __name__ == '__main__':
    # 如何爬取图片数据
    url = 'https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1602353657924&di=4b05871977b889188ce2bde490659196&imgtype=0&src=http%3A%2F%2Fa3.att.hudong.com%2F14%2F75%2F01300000164186121366756803686.jpg'
    # content 返回的是二进制形式的图片数据
    # text （字符串）   content (二进制)   json() （对象）
    img_data = requests.get(url=url).content
    with open('./test/0.jpg','wb') as fp:
        fp.write(img_data)
```

#### 3. 分页图片爬取

找到 url 特征，参数化url

```python
import requests
import re
import os

if __name__ == '__main__':
    if not os.path.exists('./qiutu'):
        os.mkdir('./qiutu')
    url = 'https://www.qiushibaike.com/imgrank/page/%d/'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    for pageNum in range(1,3):
        new_url = format(url%pageNum)
        # 通用爬虫爬取对应 url 一整张页面
        page_text = requests.get(url=url, headers=headers).text
        # print(page_text)
        # 聚焦爬虫对图片进行对应解析提取
        ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
        img_src_list = re.findall(ex, page_text, re.S)
        # print(img_src_list)
        for src in img_src_list:
            # 拼接一个完整的图片 url
            src = 'https:' + src
            # 请求到了图片的二进制数据
            img_data = img_data = requests.get(url=src, headers=headers).content
            # 生成图片名称
            img_name = src.split('/')[-1]
            # print(img_name)
            # 图片存储路径
            img_path = './qiutu/' + img_name
            with open(img_path, 'wb') as fp:
                fp.write(img_data)
                print(img_name, '下载成功！')
```

#### 4. bs4 解析器基础

数据解析原理：

* 标签定位
* 提取标签，标签属性中存储的数据值

`bs4` 进行数据解析原理：

* 实例化一个 `BeautifulSoup` 对象，并且将页面源码数据加载到该对象中。
* 通过调用 `BeautifulSoup` 对象中相关的属性或者方法进行标签定位和数据提取

环境安装

```bash
pip install bs4
pip install lxml
```

如何实例化 `BeautifulSoup` 对象。

* 将本地的 html 文档中的数据加载到该对象中去

  ```python
  from bs4 import BeautifulSoup
  
  if __name__ == '__main__':
      with open('./test/test.txt','r',encoding='utf-8') as fp:
          soup = BeautifulSoup(fp,'lxml')
          print(soup)
  
  ```

* 将互联网上获取的页面源码加载到该对象中去

  ```python
  page_text = reponse.text
  soup = BeautifulSoup(page_text,'lxml')
  ```

提供用于数据解析的方法和定位：

* `soup.tagName ` 返回的是文档中第一次出现的  tagName 对应的标签。
* `soup.find()` 
  * find('tagName')    等同于 soup.tagName
  * 属性定位： soup.find('div'),class_/id/attr='song')
* `soup.findall('tagName')`:  返回符合要求的所有标签
* `soup.select('某种选择器 id, class, 标签...选择器')` 返回的是一个列表
  * 层级选择器：
    * `soup.select('.poem-detail-item-content > p > span')` : > 表示一个层级
    * `soup.select('.poem-detail-item-content span')`: 空格 表示多个层级
* 获取标签之间的文本数据
  * `soup.a.text/string/get_text()` 
  * text/get_text()  :  可以获取某一个标签中所有的文本内容·
  * string : 只可以获取该标签下面直系的文本内容
* 获取标签中属性值
  * `soup.a['href']` : 将定位到标签中对应属性值。



```python
# 需求： 爬取三国演义小说所有的标题和章节内容
# https://www.shicimingju.com/book/sanguoyanyi.html

import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 对首页的数据进行爬取
    url = 'https://www.shicimingju.com/book/sanguoyanyi.html'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    page_text = requests.get(url=url,headers=headers).text
    # print(page_text)
    # 在首页中解析出章节的标题和详情页 url
    # 1. 实例化 BeautifulSoup 对象，需要将页面源码数据加载到该对象中
    soup = BeautifulSoup(page_text,'lxml')
    # 解析章节标题和详情页的 url
    li_list = soup.select('.book-mulu > ul > li')
    # print(li_list)
    fp = open('./sanguo.txt','w',encoding='utf-8')

    for li in li_list:
        # print(li.a)
        title = li.a.string
        detail_url = 'https://www.shicimingju.com'+li.a['href']
        # 对详情页发起请求，解析出章节内容
        detail_page_text = requests.get(url=detail_url,headers=headers).text
        # 解析出详情页相关的章节内容
        detail_soup = BeautifulSoup(detail_page_text,'lxml')
        div_tag_content = detail_soup.find('div',class_='chapter_content').text
        # print(div_tag)
        fp.write(title+':'+div_tag_content+'\n')
        print(title,'爬取成功！')
```

#### 5. xpath 解析

`xpath 解析`: 最常用且最便捷的高效的一种解析方式，通用性。

**原理：** 

1. 实例化一个 `etree` 的对象，且需要将被解析的页面源码数据加载到该对象中去。
2. 调用 `etree` 对象中的xpath 方法结合着 xpath 表达式实现标签的定位和内容的捕获。

**环境的安装：**

```bath
pip install lxml
```

**实例化etree对象：**

1. 将本地的 html 文档中的源码数据加载到 etree 对象中去

   ```python
   from lxml inport stree
   etree.parse(filePath)
   ```

2. 可以将从互联网上获取的源码数据加载到对象中

   ```python
   stree.HTML('page_text')
   ```

3. xpath('xpath表达式')

**xpath 表达式**

1.  `/` : 表示的是从根节点开始定位，表示的是一个层级
2.  `//` :  表示的是多个层级，可以表示从任意位置开始定位
3. 属性定位： //div[@class="属性值"]   tag[@id="值"]
4. 索引定位：  //div[@class="值"]/p[3]    索引从 1 开始的
5. 取文本
   	* `/text()`  获取的是标签中直系的文本内容
      	* `//text()`  获取的是标签中所有的文本内容
6. 取属性
   *  `/@attrName`  ==>    img/@src

```python
from lxml import etree

if __name__ == '__main__':
    # 实例化好了一个对象，且将被解析的源码加载到了对象中
    # tree = etree.parse('./test/test.html')
    # 在使用etree.parse时报错，原因：该方法默认使用的是“XML”解析器，所以如果碰到不规范的html文件时就会解析错误
    parser = etree.HTMLParser(encoding='utf-8')
    tree = etree.parse('./test/test.html', parser=parser)
    # r = tree.xpath('/html/body/div')    # 找到html标签 body 下的 div 标签
    # r = tree.xpath('/html//div')         # 找到 html 下的所有 div 标签
    # r = tree.xpath('//div')               # 找到 所有 div 标签
    # r = tree.xpath('//div[@class="poem-detail-item-content"]/p[@class="poem-detail-main-text"]'
    #               '/span[@id="body_1_0"]/em/span/text()')
    #r = tree.xpath('//div[@class="poem-detail-item-content"]/p[@class="poem-detail-main-text"]'
    #               '/span[@id="body_1_0"]/em//text()')
    r = tree.xpath('//div[@class="poem-detail-item-content"]/div/@class')
    print(r)

```

#### xpath爬取二手房信息

```python
# 需求： 爬取58二手房中的
# https://bj.58.com/ershoufang/

import requests
from lxml import etree

if __name__ == '__main__':
    url = 'https://bj.58.com/ershoufang/'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    page_text = requests.get(url=url,headers=headers).text

    # 数据解析
    tree = etree.HTML(page_text)
    # print(tree)
    li_list = tree.xpath('//ul[@class="house-list-wrap"]/li[@class="sendsoj"]')
    # li_list = tree.xpath('//h2[@class="title"]')
    # print(li_list)
    for li in li_list:
        r = li.xpath('./div[2]/h2/a/text()')[0]
        # r = li.xpath('./a/text()')
        print(r)
```

#### xpath分页爬取图片

```python
# http://pic.netbian.com/4kfengjing/
# 需求: 爬取图片

import requests
from lxml import etree
import os

if __name__ == '__main__':
    url = 'http://pic.netbian.com/4kfengjing/index_%d.html'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    for num in range(1, 5):
        if num==1:
            new_url='http://pic.netbian.com/4kfengjing/'
        else:
            new_url = format(url%num)

        reponse = requests.get(url=new_url,headers=headers)
        reponse.encoding='gbk'
        page_text = reponse.text

        # 数据解析:  src 属性值  alt 属性
        tree = etree.HTML(page_text)
        li_list = tree.xpath('//div[@class="slist"]/ul/li')

        # 创建一个文件夹
        if not os.path.exists('./picLibs'):
            os.mkdir('./picLibs')
        for li in li_list:
            img_src = 'http://pic.netbian.com' + li.xpath('.//a/img/@src')[0]
            img_name = li.xpath('.//a/img/@alt')[0] + '.jpg'
            # print(img_src)
            # print(img_name)

            # 请求图片进行持久性存储
            img_data = requests.get(url=img_src,headers=headers).content
            img_path = 'picLibs/' + img_name
            # print(img_path)
            with open(img_path, 'wb') as fp:
                fp.write(img_data)
                print(img_name,'下载成功！！！')
```

#### xpath爬取全国城市名称

```python
# 需求： 解析出所有城市名称：解析出所有城市名称 https://www.aqistudy.cn/historydata

import requests
from lxml import etree

if __name__ == '__main__':
    url = 'https://www.aqistudy.cn/historydata'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    page_text = requests.get(url=url,headers=headers).text
    tree = etree.HTML(page_text)
    li_list = tree.xpath('//div[@class="bottom"]/ul[@class="unstyled"]/li/a/text()')
    # print(li_list)
    for li in li_list:
        print(li)       # 打印热门城市
    all_li_list = tree.xpath('//div[@class="bottom"]/ul[@class="unstyled"]/div[2]/li/a/text()')
    # print(all_li_list)
    for li in all_li_list:
        print(li)        # 打印全部城市

```

#### xpath爬取站长直接简历

```python
# 需求： 爬取站长之家免费素材  http://sc.chinaz.com/jianli/free.html

import requests
from lxml import etree
import os

if __name__ == '__main__':
    if not os.path.exists('./jianli'):
        os.mkdir('./jianli')
    url = 'http://sc.chinaz.com/jianli/free%d.html'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    for num in range(1,10):
        if num == 1:
            new_url = 'http://sc.chinaz.com/jianli/free.html'
        else:
            new_url = format(url%num)
        page_text = requests.get(url=new_url,headers=headers).text
        # print(page_text)
        tree = etree.HTML(page_text)
        div_list =tree.xpath('//div[@class="sc_warp  mt20"]/div/div/div/a/@href')
        # print(div_list)
        for list_url in div_list:
            page_text = requests.get(url=list_url,headers=headers).content
            tree = etree.HTML(page_text)
            div_name = tree.xpath('//div[@class="ppt_tit clearfix"]/h1/text()')[0]
            # print(div_name)
            down_url = tree.xpath('//div[@class="clearfix mt20 downlist"]/ul/li/a/@href')[0]
            # print(div_list)
            rar_data = requests.get(url=down_url,headers=headers).content
            rar_path = './jianli/'+div_name+'.rar'
            with open(rar_path,'wb') as fp:
                fp.write(rar_data)
                print(rar_path,'下载成功！！！')
                fp.close()
```

## 六、验证码

#### 验证码识别

反扒机制：验证码，识别验证码图片中的数据，用于模拟登陆操作。

识别验证码操作：

* 人工肉眼识别。
* 第三方自动识别

####

http://www.ttshitu.com/



## 七、高性能异步爬虫

目的: 在爬虫中使用异步实现高性能的数据爬取操作。

#### 1. 同步爬虫

```python
import requests
headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
}

urls = {
    'http://sc.chinaz.com/jianli/201024287060.htm',
    'http://sc.chinaz.com/jianli/201024404461.htm',
    'http://sc.chinaz.com/jianli/201023166601.htm'
}

def get_content(url):
    print('正在爬取',url)
    response = requests.get(url=url,headers=headers)
    if response.status_code == 200:
        return response.content

def parse_content(content):
    print('响应数据的ch长度为： ',len(content))

for url in urls:
    content = get_content(url)
    parse_content(content)
```

会逐个请求每个url，速度会比较慢。

#### 2. 异步爬虫

异步爬虫方式：

* 多线程、多进程（不建议）
  * 好处： 可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。
  * 坏处： 无法无限制的开启多线程或者多进程。
  
* 线程池、进程池 （适当使用）

  * 好处：我们可以降低系统对进程或进程创建和销毁的一个频率，从而很好的降低系统的开销。
  * 弊端： 池中进程或线程的数量是有上限的。

  单线程：

  ```python
  import time
  # 使用单线程串行方式执行
  
  def get_page(str):
      print('正在下载： ',str)
      time.sleep(2)
      print('下载成功：  ',str)
  
  name_list = ['xiaozi','aa','bb','cc']
  
  start_time = time.time()
  
  for name in name_list:
      get_page(name)      
  
  end_time = time.time()
  # 4 个元素，8秒
  print('%d second'%(end_time-start_time))
  ```

  线程池：

  ```python
  # 使用线程池方式
  import time
  # 导入线程池模块对应类
  from multiprocessing.dummy import Pool
  
  def get_page(str):
      print('正在下载   ',str)
      time.sleep(2)
      print('下载完成    ',str)
  
  name_list = ['aa','bb','cc','dd']
  
  start_time = time.time()
  
  # 实例化一个线程池对象
  pool = Pool(4)
  # 将列表中每一个列表元素传递给 get_page 进行处理
  pool.map(get_page,name_list)
  
  end_time = time.time()
  
  # 利用线程池，提升速度，变为 2 秒
  print('%d second'%(end_time-start_time))
  ```
#### 3.线程池在爬虫中的应用

## 八、协程

协程不是计算机提供，而是程序员认为创造。

`协程` (Coroutine) ,也可以被称为微线程，是一种用户态的上下文切换技术。简而言之，其实就是通过一个线程实现代码块相互切换执行。例如：

```python
def func():
    print(1)
    ...
    print(2)
    
def func2():
    print(3)
    ...
    print(4)
    
func1()
func2()
```

实现协程的几种方法：

* greenlet, 早期模块
* yield 关键字
* asyncio 装饰器（py3.4）
* asyn 、await 关键字（py3.5）[推荐]

#### 1. greenlet 实现协程

```cmd
pip3 install greenlet
```

```python
from greenlet import greenlet

def func1():
    print(1)        # 第1步：输出1
    gr2.switch()    # 第3步：切换到 func2 函数
    print(2)        # 第6步：输出2
    gr2.switch()    # 第7步：切换到 func2 函数，从上一次执行的位置继续向后执行

def func2():
    print(3)        # 第4步：输出3
    gr1.switch()    # 第5步：切换到 func1 函数，从上一次执行的位置继续向后执行
    print(4)        # 第8步：输出4

gr1 = greenlet(func1)
gr2 = greenlet(func2)

gr1.switch()      # 第一步： 去执行 func1 函数
```

#### 2. yield 关键字

```python
def func1():
    yield 1
    yield from func2()
    yield 2
    
def func():
    yield 3
    yield 4
   
f1 = func1()
for item in f1:
    print(item)
```

#### 3. asyncio

在  python3.4 及之后的版本。

```python
import asyncio

@asyncio.coroutine
def func1():
    print(1)
    yield from asyncio.sleep(2)  # 遇到 IO 耗时操作，自动切换到taks中的其他任务
    print(2)
   
@asyncio.contine
def func2():
    print(3)
    yield from asyncio.sleep(2) # 遇到 IO 耗时操作，自动切换到tasls中的其他任务
    print(4)
   
tasks=[
    asyncio.ensure_future(func1()),
    asyncio.ensure_future(func2())
]

loop = asyncio.get_future
loop.run_until_complete(asyncio.wait(tasks))
    
```



