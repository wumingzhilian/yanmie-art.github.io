## 一、爬虫技术相关库

网络请求：

* urllib
* requests / urllib3
* selenium ( UI 自动测试、动态js渲染 )
* appium (手机 app 的爬虫或 UI 测试)
* re 正则 
* xpath
* bs4
* json

数据存储：

* pymysql
* mongodb
* elasticsearch

多任务库：

* 多线程 （threading）、线程队列 （queue）
* 协程（asynio、gevent / eventlet）

爬虫框架：

* scrapy
* scrapy-redis 分布式（多机爬虫）

## 二、常见的反爬策略

* UA 策略
* 登录限制（cookie）策略
* 亲求频率（IP代理） 策略
* 验证码（图片 - 云打码，文字或物件图片验证，滑块）
* 动态 js （selenium / splash / api 接口）策略

## 三、绕过UA检测

假如我们要爬取一个网站，就用本地来测试把，先来看看直接爬取的时候中间件记录的日志。

```python
import requests

if __name__ == "__main__":
    # 指定url
    url = 'http://127.0.0.1/'
    # 发起请求
    response = requests.get(url=url)
    # 获取响应数据
    page_text = response.text
    print(page_text)
    with open('./pachong.html','w',encoding='utf-8') as fp:
        fp.write(page_text)
    print("爬取数据结束")
```

可以从日志中看到，ua是 python 库的标志。

```python
127.0.0.1 - - [07/Oct/2020:19:56:34 +0800] "GET / HTTP/1.1" 200 89796 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
127.0.0.1 - - [07/Oct/2020:20:46:19 +0800] "GET / HTTP/1.1" 200 88896 "-" "python-requests/2.24.0"
```

假如网站加入了UA检测策略，这个当然也就爬取不出数据来了。

所以可以伪造UA，既可以成功绕过了。

```python
import requests

if __name__ == "__main__":
    # 指定url
    url = 'http://127.0.0.1/'
    headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    # 发起请求
    response = requests.get(url=url,headers=headers)
    # 获取响应数据
    page_text = response.text
    print(page_text)
    with open('./pachong.html','w',encoding='utf-8') as fp:
        fp.write(page_text)
    print("爬取数据结束")
```

记录结果：

```bash
127.0.0.1 - - [07/Oct/2020:20:46:19 +0800] "GET / HTTP/1.1" 200 88896 "-" "python-requests/2.24.0"
127.0.0.1 - - [07/Oct/2020:20:55:25 +0800] "GET / HTTP/1.1" 200 88951 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
```

## 四、requests库

#### requests之网页采集

```python
import requests

if __name__ == "__main__":
    url = 'https://www.sogou.com/web'
    headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    # 处理 url 携带的参数 ： 封装到字典中
    kw =  input('enter a word: ')
    param = {
        'query':kw
    }
    respose = requests.get(url=url,params=param,headers=headers)
    page_text = respose.text
    print(page_text)
    filename = kw+'.html'
    with open(filename,'w',encoding='utf-8') as fp:
        fp.write(page_text)
    print(filename+'  保存成功！！！')

```

#### requests之百度翻译破解

当进行百度翻译时，我们发现输入但此后，不点翻译按钮，他也会自动显示翻译结果，也就是进行局部刷新( ajax请求 )。

![0dgFTe.png](https://s1.ax1x.com/2020/10/07/0dgFTe.png)

![0dgAFH.png](https://s1.ax1x.com/2020/10/07/0dgAFH.png)

![image-20201007210827117](C:%5CUsers%5C51946%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20201007210827117.png)

发送了三个`d`、`do`、`dog`的post包。

我们每录入一个字符就会进行一次ajax请求。

![0dgYpn.png](https://s1.ax1x.com/2020/10/07/0dgYpn.png)

返回一串json数据。

```python

import requests
import json
if __name__ == "__main__":
    post_url = "https://fanyi.baidu.com/sug"
    # UA 伪装
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    # post 请求参数处理（通get）请求一致
    word = input('enter a word: ')
    data = {
        'kw':word
    }
    # 请求发送
    reponse = requests.post(url=post_url,data=data,headers=headers)
    # 获取响应数据
    # 直接打印 josn 数据
    #print(reponse.text)
    # json() 方法返回的是 obj （确认服务器响应数据类型是 josn 数据，才能使用json方法）
    dic_obj = reponse.json()
    print(dic_obj)
    # 进行持久化存储
    fp = open('./'+word+'.json','w',encoding='utf-8')
    json.dump(dic_obj,fp,ensure_ascii=False)
    print('over!')
```

#### requests之豆瓣电影

https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7%E7%89%87&type=24&interval_id=100:90&action=

当滚轮拖动到底部的时候，有变到了中间，就可以继续网下滑了。也是当滑轮到达底部的时候进行了 ajax 请求。

![0dWgcn.png](https://s1.ax1x.com/2020/10/07/0dWgcn.png)

```python
import requests
import json

if __name__ == '__main__':
    url = 'https://movie.douban.com/j/chart/top_list'
    param = {
        'type':'24',
        'interval_id':'100:90',
        'action':'',
        'start':'1',    # 从库中的第几部开始取
        'limit':'20',    # 一次取出来多少
    }
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    reponse = requests.get(url=url,params=param,headers=headers)
    list_data = reponse.json()
    fp = open('./douban.json','w',encoding='utf-8')
    json.dump(list_data,fp=fp,ensure_ascii=False)

    print('over!!!')
```

#### requests之餐厅爬取

这里查询也是 ajax 请求

http://www.kfc.com.cn/kfccda/storelist/index.aspx

![0dIKoV.png](https://s1.ax1x.com/2020/10/07/0dIKoV.png)

```python
import requests

if __name__ == '__main__':
    url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword'
    headers={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    data = {
        'cname':'',
        'pid':'',
        'keyword':'北京',
        'pageIndex':'1',
        'pageSize':'10',
    }
    reponse = requests.post(url=url,data=data,headers=headers)
    list_data = reponse.json()
    print(list_data)

```

#### requests之药监总局



* 需求：爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据 http://scxk.nmpa.gov.cn:81/xk/

```python
import requests
import json

if __name__ == '__main__':
    url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    data={
        'on':'true',
        'page':'1',
        'pageSize':'15',
        'productName':'',
        'conditionType':'1',
        'applyname':'',
        'applysn':''
    }
    json_ids = requests.post(url=url,data=data,headers=headers).json()
    id_list = []    # 存储企业 id
    all_data_list = []   # 存储所有的企业详情数据
    for dict in json_ids['list']:
        # print(dict['ID'])
        id_list.append(dict['ID'])

    post_url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
    for id in id_list:
        # print(id)
        data ={
            'id':id
        }
        detail_json = requests.post(url=post_url,data=data,headers=headers).json()
        all_data_list.append(detail_json)
    # print(all_data_list)
    fp = open('./test/allData.json','w',encoding='utf-8')
    json.dump(all_data_list,fp=fp,ensure_ascii=False)
    print('over!!!')
```

## 五、数据解析

聚焦爬虫：爬取页面中指定页面内容。

编码流程：

* 指定 url
* 发起请求
* 获取响应数据
* 数据解析
* 持久化存储

数据解析分类：

* 正则
* bs4
* xpath (***)

数据解析原理概述

* 解析的局部文本内容都会在标签或者标签对应的属性中存储
  * 进行指定标签的定位
  * 标签或者标签对应的属性中存储的数据值进行提取（解析）

#### 1. 数据解析--正则表达式

![0w06F1.png](https://s1.ax1x.com/2020/10/08/0w06F1.png)

#### 2. 图片爬取

```python
import requests
if __name__ == '__main__':
    # 如何爬取图片数据
    url = 'https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1602353657924&di=4b05871977b889188ce2bde490659196&imgtype=0&src=http%3A%2F%2Fa3.att.hudong.com%2F14%2F75%2F01300000164186121366756803686.jpg'
    # content 返回的是二进制形式的图片数据
    # text （字符串）   content (二进制)   json() （对象）
    img_data = requests.get(url=url).content
    with open('./test/0.jpg','wb') as fp:
        fp.write(img_data)
```

#### 3. 分页图片爬取

找到 url 特征，参数化url

```python
import requests
import re
import os

if __name__ == '__main__':
    if not os.path.exists('./qiutu'):
        os.mkdir('./qiutu')
    url = 'https://www.qiushibaike.com/imgrank/page/%d/'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    for pageNum in range(1,3):
        new_url = format(url%pageNum)
        # 通用爬虫爬取对应 url 一整张页面
        page_text = requests.get(url=url, headers=headers).text
        # print(page_text)
        # 聚焦爬虫对图片进行对应解析提取
        ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
        img_src_list = re.findall(ex, page_text, re.S)
        # print(img_src_list)
        for src in img_src_list:
            # 拼接一个完整的图片 url
            src = 'https:' + src
            # 请求到了图片的二进制数据
            img_data = img_data = requests.get(url=src, headers=headers).content
            # 生成图片名称
            img_name = src.split('/')[-1]
            # print(img_name)
            # 图片存储路径
            img_path = './qiutu/' + img_name
            with open(img_path, 'wb') as fp:
                fp.write(img_data)
                print(img_name, '下载成功！')
```

#### 4. bs4 解析器基础

数据解析原理：

* 标签定位
* 提取标签，标签属性中存储的数据值

`bs4` 进行数据解析原理：

* 实例化一个 `BeautifulSoup` 对象，并且将页面源码数据加载到该对象中。
* 通过调用 `BeautifulSoup` 对象中相关的属性或者方法进行标签定位和数据提取

环境安装

```bash
pip install bs4
pip install lxml
```

如何实例化 `BeautifulSoup` 对象。

* 将本地的 html 文档中的数据加载到该对象中去

  ```python
  from bs4 import BeautifulSoup
  
  if __name__ == '__main__':
      with open('./test/test.txt','r',encoding='utf-8') as fp:
          soup = BeautifulSoup(fp,'lxml')
          print(soup)
  
  ```

* 将互联网上获取的页面源码加载到该对象中去

  ```python
  page_text = reponse.text
  soup = BeautifulSoup(page_text,'lxml')
  ```

提供用于数据解析的方法和定位：

* `soup.tagName ` 返回的是文档中第一次出现的  tagName 对应的标签。
* `soup.find()` 
  * find('tagName')    等同于 soup.tagName
  * 属性定位： soup.find('div'),class_/id/attr='song')
* `soup.findall('tagName')`:  返回符合要求的所有标签
* `soup.select('某种选择器 id, class, 标签...选择器')` 返回的是一个列表
  * 层级选择器：
    * `soup.select('.poem-detail-item-content > p > span')` : > 表示一个层级
    * `soup.select('.poem-detail-item-content span')`: 空格 表示多个层级
* 获取标签之间的文本数据
  * `soup.a.text/string/get_text()` 
  * text/get_text()  :  可以获取某一个标签中所有的文本内容·
  * string : 只可以获取该标签下面直系的文本内容
* 获取标签中属性值
  * `soup.a['href']` : 将定位到标签中对应属性值。



```python
# 需求： 爬取三国演义小说所有的标题和章节内容
# https://www.shicimingju.com/book/sanguoyanyi.html

import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 对首页的数据进行爬取
    url = 'https://www.shicimingju.com/book/sanguoyanyi.html'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    page_text = requests.get(url=url,headers=headers).text
    # print(page_text)
    # 在首页中解析出章节的标题和详情页 url
    # 1. 实例化 BeautifulSoup 对象，需要将页面源码数据加载到该对象中
    soup = BeautifulSoup(page_text,'lxml')
    # 解析章节标题和详情页的 url
    li_list = soup.select('.book-mulu > ul > li')
    # print(li_list)
    fp = open('./sanguo.txt','w',encoding='utf-8')

    for li in li_list:
        # print(li.a)
        title = li.a.string
        detail_url = 'https://www.shicimingju.com'+li.a['href']
        # 对详情页发起请求，解析出章节内容
        detail_page_text = requests.get(url=detail_url,headers=headers).text
        # 解析出详情页相关的章节内容
        detail_soup = BeautifulSoup(detail_page_text,'lxml')
        div_tag_content = detail_soup.find('div',class_='chapter_content').text
        # print(div_tag)
        fp.write(title+':'+div_tag_content+'\n')
        print(title,'爬取成功！')
```

#### 5. xpath 解析

`xpath 解析`: 最常用且最便捷的高效的一种解析方式，通用性。

**原理：** 

1. 实例化一个 `etree` 的对象，且需要将被解析的页面源码数据加载到该对象中去。
2. 调用 `etree` 对象中的xpath 方法结合着 xpath 表达式实现标签的定位和内容的捕获。

**环境的安装：**

```bath
pip install lxml
```

**实例化etree对象：**

1. 将本地的 html 文档中的源码数据加载到 etree 对象中去

   ```python
   from lxml inport stree
   etree.parse(filePath)
   ```

2. 可以将从互联网上获取的源码数据加载到对象中

   ```python
   stree.HTML('page_text')
   ```

3. xpath('xpath表达式')

**xpath 表达式**

1.  `/` : 表示的是从根节点开始定位，表示的是一个层级
2.  `//` :  表示的是多个层级，可以表示从任意位置开始定位
3. 属性定位： //div[@class="属性值"]   tag[@id="值"]
4. 索引定位：  //div[@class="值"]/p[3]    索引从 1 开始的
5. 取文本
   	* `/text()`  获取的是标签中直系的文本内容
   	* `//text()`  获取的是标签中所有的文本内容
6. 取属性
   *  `/@attrName`  ==>    img/@src

```python
from lxml import etree

if __name__ == '__main__':
    # 实例化好了一个对象，且将被解析的源码加载到了对象中
    # tree = etree.parse('./test/test.html')
    # 在使用etree.parse时报错，原因：该方法默认使用的是“XML”解析器，所以如果碰到不规范的html文件时就会解析错误
    parser = etree.HTMLParser(encoding='utf-8')
    tree = etree.parse('./test/test.html', parser=parser)
    # r = tree.xpath('/html/body/div')    # 找到html标签 body 下的 div 标签
    # r = tree.xpath('/html//div')         # 找到 html 下的所有 div 标签
    # r = tree.xpath('//div')               # 找到 所有 div 标签
    # r = tree.xpath('//div[@class="poem-detail-item-content"]/p[@class="poem-detail-main-text"]'
    #               '/span[@id="body_1_0"]/em/span/text()')
    #r = tree.xpath('//div[@class="poem-detail-item-content"]/p[@class="poem-detail-main-text"]'
    #               '/span[@id="body_1_0"]/em//text()')
    r = tree.xpath('//div[@class="poem-detail-item-content"]/div/@class')
    print(r)

```

#### xpath爬取二手房信息

```python
# 需求： 爬取58二手房中的
# https://bj.58.com/ershoufang/

import requests
from lxml import etree

if __name__ == '__main__':
    url = 'https://bj.58.com/ershoufang/'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    page_text = requests.get(url=url,headers=headers).text

    # 数据解析
    tree = etree.HTML(page_text)
    # print(tree)
    li_list = tree.xpath('//ul[@class="house-list-wrap"]/li[@class="sendsoj"]')
    # li_list = tree.xpath('//h2[@class="title"]')
    # print(li_list)
    for li in li_list:
        r = li.xpath('./div[2]/h2/a/text()')[0]
        # r = li.xpath('./a/text()')
        print(r)
```

#### xpath分页爬取图片

```python
# http://pic.netbian.com/4kfengjing/
# 需求: 爬取图片

import requests
from lxml import etree
import os

if __name__ == '__main__':
    url = 'http://pic.netbian.com/4kfengjing/index_%d.html'
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0"
    }
    for num in range(1, 5):
        if num==1:
            new_url='http://pic.netbian.com/4kfengjing/'
        else:
            new_url = format(url%num)

        reponse = requests.get(url=new_url,headers=headers)
        reponse.encoding='gbk'
        page_text = reponse.text

        # 数据解析:  src 属性值  alt 属性
        tree = etree.HTML(page_text)
        li_list = tree.xpath('//div[@class="slist"]/ul/li')

        # 创建一个文件夹
        if not os.path.exists('./picLibs'):
            os.mkdir('./picLibs')
        for li in li_list:
            img_src = 'http://pic.netbian.com' + li.xpath('.//a/img/@src')[0]
            img_name = li.xpath('.//a/img/@alt')[0] + '.jpg'
            # print(img_src)
            # print(img_name)

            # 请求图片进行持久性存储
            img_data = requests.get(url=img_src,headers=headers).content
            img_path = 'picLibs/' + img_name
            # print(img_path)
            with open(img_path, 'wb') as fp:
                fp.write(img_data)
                print(img_name,'下载成功！！！')
```

